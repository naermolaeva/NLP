{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание № 3. Векторные представления"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Преобразуйте тексты в векторы в каждой паре 4 методами  - SVD, NMF, Word2Vec, Fastext. Для SVD и NMF сделайте две пары векторов - через TfidfVectorizer и CountVectorizer. Для word2vec сделайте две пары векторов - с взвешиванием по tfidf и без. Для Fastext постройте две модели - без нормализации и с нормализацией, а через каждую модель постройте две пары векторов -  с взвешиванием по tfidf и без. Для обучения этих моделей можете воспользоваться корпусом новостных текстов, с которым мы работали на семинаре. А можете использовать любой другой корпус (сами тексты соревнования использовать не надо).\n",
    "\n",
    "2. У вас должно получиться 10 пар векторов для каждой строчки в датасете. Между векторами каждой пары вычислите косинусную близость (получится 10 чисел для каждой пары текстов). \n",
    "\n",
    "3. Постройте обучающую выборку из этих близостей. Обучите любую модель (Логрег, Рандом форест или что-то ещё) на этой выборке и оцените качество на кросс-валидации (используйте микросреднюю f1-меру).   \n",
    "\n",
    "4. С помощью кросс-валидации подберите параметры моделей (количество компонент, размерность в w2v, min_n - в fastext и т.д)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lxml import html\n",
    "from string import punctuation\n",
    "from pymystem3 import Mystem\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter,defaultdict\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "russian_stopwords = set(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punct = punctuation + '«»—…“”*№–'\n",
    "mystem = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    tokens = mystem.lemmatize(text)\n",
    "    tokens = [token for token in tokens if token not in russian_stopwords \\\n",
    "              and token != ' '  \\\n",
    "              and token.strip() not in punct]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_xml = html.fromstring(open('paraphraser/paraphrases.xml', 'rb').read())\n",
    "texts_1 = []\n",
    "texts_2 = []\n",
    "classes = []\n",
    "\n",
    "for p in corpus_xml.xpath('//paraphrase'):\n",
    "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
    "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
    "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
    "    \n",
    "data = pd.DataFrame({'text_1' : texts_1, 'text_2' : texts_2, 'label' : classes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_1_norm'] = data['text_1'].apply(normalize)\n",
    "data['text_2_norm'] = data['text_2'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>label</th>\n",
       "      <th>text_1_norm</th>\n",
       "      <th>text_2_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Полицейским разрешат стрелять на поражение по ...</td>\n",
       "      <td>Полиции могут разрешить стрелять по хулиганам ...</td>\n",
       "      <td>0</td>\n",
       "      <td>полицейский разрешать стрелять поражение гражд...</td>\n",
       "      <td>полиция мочь разрешать стрелять хулиган травма...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Право полицейских на проникновение в жилище ре...</td>\n",
       "      <td>Правила внесудебного проникновения полицейских...</td>\n",
       "      <td>0</td>\n",
       "      <td>право полицейский проникновение жилище решать ...</td>\n",
       "      <td>правило внесудебный проникновение полицейский ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Президент Египта ввел чрезвычайное положение в...</td>\n",
       "      <td>Власти Египта угрожают ввести в стране чрезвыч...</td>\n",
       "      <td>0</td>\n",
       "      <td>президент египет вводить чрезвычайный положени...</td>\n",
       "      <td>власть египет угрожать вводить страна чрезвыча...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Вернувшихся из Сирии россиян волнует вопрос тр...</td>\n",
       "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
       "      <td>-1</td>\n",
       "      <td>вернуться сирия россиянин волновать вопрос тру...</td>\n",
       "      <td>самолет мчс вывозить россиянин разрушать сирия</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>В Москву из Сирии вернулись 2 самолета МЧС с р...</td>\n",
       "      <td>Самолеты МЧС вывезут россиян из разрушенной Си...</td>\n",
       "      <td>0</td>\n",
       "      <td>москва сирия вернуться 2 самолет мчс россиянин...</td>\n",
       "      <td>самолет мчс вывозить россиянин разрушать сирия</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_1  \\\n",
       "0  Полицейским разрешат стрелять на поражение по ...   \n",
       "1  Право полицейских на проникновение в жилище ре...   \n",
       "2  Президент Египта ввел чрезвычайное положение в...   \n",
       "3  Вернувшихся из Сирии россиян волнует вопрос тр...   \n",
       "4  В Москву из Сирии вернулись 2 самолета МЧС с р...   \n",
       "\n",
       "                                              text_2 label  \\\n",
       "0  Полиции могут разрешить стрелять по хулиганам ...     0   \n",
       "1  Правила внесудебного проникновения полицейских...     0   \n",
       "2  Власти Египта угрожают ввести в стране чрезвыч...     0   \n",
       "3  Самолеты МЧС вывезут россиян из разрушенной Си...    -1   \n",
       "4  Самолеты МЧС вывезут россиян из разрушенной Си...     0   \n",
       "\n",
       "                                         text_1_norm  \\\n",
       "0  полицейский разрешать стрелять поражение гражд...   \n",
       "1  право полицейский проникновение жилище решать ...   \n",
       "2  президент египет вводить чрезвычайный положени...   \n",
       "3  вернуться сирия россиянин волновать вопрос тру...   \n",
       "4  москва сирия вернуться 2 самолет мчс россиянин...   \n",
       "\n",
       "                                         text_2_norm  \n",
       "0  полиция мочь разрешать стрелять хулиган травма...  \n",
       "1  правило внесудебный проникновение полицейский ...  \n",
       "2  власть египет угрожать вводить страна чрезвыча...  \n",
       "3     самолет мчс вывозить россиянин разрушать сирия  \n",
       "4     самолет мчс вывозить россиянин разрушать сирия  "
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7227,)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data['label'].values\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучающий корпус**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_rt = pd.read_csv('news_texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_rt.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rt['content_norm'] = data_rt['content'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>content_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Канцлер Германии Ангела Меркель в ходе брифинг...</td>\n",
       "      <td>канцлер германия ангел меркель ход брифинг пре...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Российские и белорусские войска успешно заверш...</td>\n",
       "      <td>российский белорусский войско успешно завершат...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Дзюба, Шатов и Анюков оказались не нужны «Зени...</td>\n",
       "      <td>дзюба шатов анюков оказываться нужный зенит ро...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>В Испанию без фанатов\\nПожалуй, главной пятнич...</td>\n",
       "      <td>испания фанат пожалуй главный пятничный новост...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Постпред России при ООН Виталий Чуркин, говоря...</td>\n",
       "      <td>постпред россия оон виталий чуркин говорить ве...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Канцлер Германии Ангела Меркель в ходе брифинг...   \n",
       "1  Российские и белорусские войска успешно заверш...   \n",
       "2  Дзюба, Шатов и Анюков оказались не нужны «Зени...   \n",
       "3  В Испанию без фанатов\\nПожалуй, главной пятнич...   \n",
       "4  Постпред России при ООН Виталий Чуркин, говоря...   \n",
       "\n",
       "                                        content_norm  \n",
       "0  канцлер германия ангел меркель ход брифинг пре...  \n",
       "1  российский белорусский войско успешно завершат...  \n",
       "2  дзюба шатов анюков оказываться нужный зенит ро...  \n",
       "3  испания фанат пожалуй главный пятничный новост...  \n",
       "4  постпред россия оон виталий чуркин говорить ве...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features=25000, min_df=5, max_df=0.3)\n",
    "X_cv = cv.fit_transform(data_rt['content_norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7212, 25000)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=25000, min_df=5, max_df=0.3)\n",
    "X_tfidf = tfidf.fit_transform(data_rt['content_norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7212, 25000)"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Матричные разложения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# svd = TruncatedSVD(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_cv = svd.fit(X_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2word = {i:w for i, w in enumerate(cv.get_feature_names())}\n",
    "word2id = {w:i for i, w in id2word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2vec_svd_cv = svd_cv.components_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.41534265e-03, -6.48203597e-03, -1.37011631e-03, ...,\n",
       "        -1.48205509e-02,  3.36881441e-03,  8.27630806e-03],\n",
       "       [ 7.13254030e-04, -1.60944585e-03, -3.96059363e-05, ...,\n",
       "         1.34776320e-04, -1.82137359e-03, -1.06975742e-03],\n",
       "       [ 3.12128376e-05, -1.50032357e-04, -1.85430737e-04, ...,\n",
       "         8.01392151e-04,  5.56368751e-04,  2.41479701e-04],\n",
       "       ...,\n",
       "       [ 1.40193677e-04, -5.29234098e-04, -8.51837961e-04, ...,\n",
       "         6.03649799e-04, -1.04440254e-04,  8.15950285e-04],\n",
       "       [ 4.18618801e-04, -7.15927320e-04,  5.19265852e-04, ...,\n",
       "        -2.09985371e-03,  1.99516325e-03, -2.10304586e-03],\n",
       "       [ 2.93899296e-04, -9.17392469e-04,  8.87922747e-04, ...,\n",
       "        -1.05162688e-03, -1.04546109e-03,  4.17432539e-04]])"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2vec_svd_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_tfidf = svd.fit(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2word_tfidf = {i:w for i, w in enumerate(tfidf.get_feature_names())}\n",
    "word2id_tfidf = {w:i for i, w in id2word_tfidf.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2vec_svd_tfidf = svd_tfidf.components_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.69172155e-03, -5.10472874e-03,  3.43478922e-03, ...,\n",
       "         1.63740964e-03, -3.94333774e-03,  4.77463889e-03],\n",
       "       [ 2.06438312e-03, -1.16265669e-03, -2.94723222e-05, ...,\n",
       "        -5.47429764e-03, -1.39607513e-03, -1.87790362e-03],\n",
       "       [ 2.12949350e-04, -1.82626523e-04,  4.25569335e-04, ...,\n",
       "         1.13159323e-03, -1.38074693e-03,  2.75504006e-04],\n",
       "       ...,\n",
       "       [ 8.95309054e-04, -6.17879668e-04,  2.08424019e-03, ...,\n",
       "         2.95955444e-04,  1.38401833e-03,  3.83496190e-04],\n",
       "       [ 1.51850048e-03, -7.88303339e-04, -8.89760545e-04, ...,\n",
       "        -1.43524020e-03, -1.57015252e-03, -6.56444224e-03],\n",
       "       [ 1.88179728e-03, -9.39920802e-04, -1.63877147e-03, ...,\n",
       "        -1.36807687e-03,  9.71108368e-04, -2.64889606e-03]])"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2vec_svd_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmf = NMF(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_cv = nmf.fit(X_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2vec_nmf_cv = nmf_cv.components_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_tfidf = nmf.fit(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2vec_nmf_tfidf = nmf_tfidf.components_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_similar(word, id2vec):\n",
    "    similar = [id2word[i] for i in cosine_distances(id2vec[word2id[word]].reshape(1, -1), id2vec).argsort()[0][:10]]\n",
    "    return similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['спорт',\n",
       " 'вид',\n",
       " 'фехтование',\n",
       " 'артистка',\n",
       " 'скалолазание',\n",
       " 'триатлон',\n",
       " 'плавание',\n",
       " 'многократный',\n",
       " 'поло',\n",
       " 'синхронистка']"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('спорт', id2vec_svd_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['спорт',\n",
       " 'спортивный',\n",
       " 'вид',\n",
       " 'колобок',\n",
       " 'карате',\n",
       " 'соревнование',\n",
       " 'спортсмен',\n",
       " 'фехтование',\n",
       " 'журов',\n",
       " 'плавание']"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('спорт', id2vec_svd_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['спорт',\n",
       " 'кратный',\n",
       " 'спортивный',\n",
       " 'пловчиха',\n",
       " 'sport',\n",
       " 'пятикратный',\n",
       " 'анфиса',\n",
       " 'анастасия',\n",
       " 'тренировочный',\n",
       " 'уподобляться']"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('спорт', id2vec_nmf_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['спорт',\n",
       " 'фехтование',\n",
       " 'спортивный',\n",
       " 'многократный',\n",
       " 'колобок',\n",
       " 'ои',\n",
       " 'гребной',\n",
       " 'допинговый',\n",
       " 'допинг',\n",
       " 'стивенс']"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('спорт', id2vec_nmf_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.base_any2vec:consider setting layer size to a multiple of 4 for greater performance\n"
     ]
    }
   ],
   "source": [
    "w2v = gensim.models.Word2Vec([text.split() for text in data_rt['content_norm']], size=50, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# w2v = gensim.models.Word2Vec([text.split() for text in data_rt['content_norm']], size=100, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('колобок', 0.8065358996391296),\n",
       " ('колобков', 0.8039644956588745),\n",
       " ('велосипедный', 0.7796916365623474),\n",
       " ('лыжный', 0.7776771187782288),\n",
       " ('фигурный', 0.7748597264289856),\n",
       " ('синхронный', 0.7699201703071594),\n",
       " ('санный', 0.7696366310119629),\n",
       " ('атлетика', 0.768450915813446),\n",
       " ('гимнастика', 0.7623220682144165),\n",
       " ('бутырский', 0.7610881924629211)]"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar('спорт')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getTF(words_arr):\n",
    "#     tf_dict = Counter(words_arr)\n",
    "    \n",
    "#     for value in tf_dict:\n",
    "#         tf_dict[value] = tf_dict[value] / float(len(words_arr))\n",
    "        \n",
    "#     return tf_dict\n",
    "\n",
    "\n",
    "# def getIDF(documents):\n",
    "#     idf_dict = defaultdict(lambda: 0)\n",
    "#     count_docs = len(documents)\n",
    "    \n",
    "#     for doc in documents:\n",
    "#         for word in doc:\n",
    "#             idf_dict[word] += 1\n",
    "#             idf_dict[word] = math.log(count_docs / float(1 + idf_dict[word]))\n",
    "        \n",
    "#     return idf_dict\n",
    "\n",
    "# def getTFIDF(documents):\n",
    "#     for doc in documents:\n",
    "#         tfidf_dict = {}\n",
    "#         tf = getTF(doc)\n",
    "        \n",
    "#         for item in tf:\n",
    "#             tfidf_dict[item] = tf[item] * float([x for x in getIDF(documents).values()][0])\n",
    "        \n",
    "#     return tfidf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**С нормализацией**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.base_any2vec:consider setting layer size to a multiple of 4 for greater performance\n"
     ]
    }
   ],
   "source": [
    "fast_text_norm = gensim.models.FastText([text.split() for text in data_rt['content_norm']], size=50, min_n=4, max_n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fast_text_norm = gensim.models.FastText([text.split() for text in data_rt['content_norm']], size=100, min_n=4, max_n=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Без нормализации**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.base_any2vec:consider setting layer size to a multiple of 4 for greater performance\n"
     ]
    }
   ],
   "source": [
    "corpus = [text.split() for text in data_rt['content'].apply(tokenize)]\n",
    "fast_text = gensim.models.FastText(corpus, size=50, min_n=4, max_n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# corpus = [text.split() for text in data_rt['content'].apply(tokenize)]\n",
    "# fast_text = gensim.models.FastText(corpus, size=100, min_n=4, max_n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('спортзале', 0.9600382447242737),\n",
       " ('велоспорт', 0.9357381463050842),\n",
       " ('спорту', 0.9300536513328552),\n",
       " ('спорте', 0.9198392629623413),\n",
       " ('спорышев', 0.8934230804443359),\n",
       " ('спорта', 0.887935996055603),\n",
       " ('спору', 0.8767692446708679),\n",
       " ('спортсмен', 0.8596939444541931),\n",
       " ('велоспорте', 0.8562847375869751),\n",
       " ('р-спорт', 0.8537545800209045)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_text.most_similar('спорт')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('спортзал', 0.9714047908782959),\n",
       " ('велоспорт', 0.9464095234870911),\n",
       " ('спортинг', 0.915722668170929),\n",
       " ('спорышев', 0.8824635744094849),\n",
       " ('автоспорт', 0.8666187524795532),\n",
       " ('мегаспорт', 0.8333557844161987),\n",
       " ('спор', 0.805184006690979),\n",
       " ('атлетика', 0.7830331921577454),\n",
       " ('рапорт', 0.7690808773040771),\n",
       " ('паралимпиец', 0.7659856677055359)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_text_norm.most_similar('спорт')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Векторные представления"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_1_cv = svd_cv.transform(cv.transform(data['text_1_norm']))\n",
    "X_text_2_cv = svd_cv.transform(cv.transform(data['text_2_norm']))\n",
    "\n",
    "X_text_cv = np.concatenate([X_text_1_cv, X_text_2_cv], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X_cv, valid_X_cv, train_y_cv, valid_y_cv = train_test_split(X_text_cv, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=100, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=500, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_cv = RandomForestClassifier(\n",
    "    max_depth=100,\n",
    "    n_estimators=500,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "clf_cv.fit(train_X_cv, train_y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`svd = TruncatedSVD(200)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro-f1 test 0.5107913669064749\n"
     ]
    }
   ],
   "source": [
    "print('micro-f1 test', metrics.f1_score(valid_y_cv, clf_cv.predict(valid_X_cv), average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`svd = TruncatedSVD(100)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro-f1 test 0.5069175428887659\n"
     ]
    }
   ],
   "source": [
    "print('micro-f1 test', metrics.f1_score(valid_y_cv, clf_cv.predict(valid_X_cv), average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_1_tfidf = svd_tfidf.transform(tfidf.transform(data['text_1_norm']))\n",
    "X_text_2_tfidf = svd_tfidf.transform(tfidf.transform(data['text_2_norm']))\n",
    "\n",
    "X_text_tfidf = np.concatenate([X_text_1_tfidf, X_text_2_tfidf], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X_tfidf, valid_X_tfidf, train_y_tfidf, valid_y_tfidf = train_test_split(X_text_tfidf, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=100, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=500, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_tfidf = RandomForestClassifier(\n",
    "    max_depth=100,\n",
    "    n_estimators=500,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "clf_tfidf.fit(train_X_tfidf, train_y_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`svd = TruncatedSVD(200)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro-f1 test 0.5179856115107914\n"
     ]
    }
   ],
   "source": [
    "print('micro-f1 test', metrics.f1_score(valid_y_tfidf, clf_tfidf.predict(valid_X_tfidf), average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`svd = TruncatedSVD(100)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro-f1 test 0.5146651909241837\n"
     ]
    }
   ],
   "source": [
    "print('micro-f1 test', metrics.f1_score(valid_y_tfidf, clf_tfidf.predict(valid_X_tfidf), average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_text_1_nmf_cv = nmf_cv.transform(cv.transform(data['text_1_norm']))\n",
    "X_text_2_nmf_cv = nmf_cv.transform(cv.transform(data['text_2_norm']))\n",
    "\n",
    "X_text_nmf_cv = np.concatenate([X_text_1_nmf_cv, X_text_2_nmf_cv], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X_nmf_cv, valid_X_nmf_cv, train_y_nmf_cv, valid_y_nmf_cv = train_test_split(X_text_nmf_cv, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=100, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=500, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_nmf_cv = RandomForestClassifier(\n",
    "    max_depth=100,\n",
    "    n_estimators=500,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "clf_nmf_cv.fit(train_X_nmf_cv, train_y_nmf_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`nmf = NMF(50)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro-f1 test 0.5146651909241837\n"
     ]
    }
   ],
   "source": [
    "print('micro-f1 test', metrics.f1_score(valid_y_nmf_cv, clf_nmf_cv.predict(valid_X_nmf_cv), average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`nmf = NMF(200)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro-f1 test 0.5279468732706143\n"
     ]
    }
   ],
   "source": [
    "print('micro-f1 test', metrics.f1_score(valid_y_nmf_cv, clf_nmf_cv.predict(valid_X_nmf_cv), average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_1_nmf_tfidf = nmf_tfidf.transform(tfidf.transform(data['text_1_norm']))\n",
    "X_text_2_nmf_tfidf = nmf_tfidf.transform(tfidf.transform(data['text_2_norm']))\n",
    "\n",
    "X_text_nmf_tfidf = np.concatenate([X_text_1_nmf_tfidf, X_text_2_nmf_tfidf], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X_nmf_tfidf, valid_X_nmf_tfidf, train_y_nmf_tfidf, valid_y_nmf_tfidf = train_test_split(\n",
    "                                                                                X_text_nmf_tfidf, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=100, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=500, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_nmf_tfidf = RandomForestClassifier(\n",
    "    max_depth=100,\n",
    "    n_estimators=500,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "clf_nmf_tfidf.fit(train_X_nmf_tfidf, train_y_nmf_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`nmf = NMF(50)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro-f1 test 0.5080243497509684\n"
     ]
    }
   ],
   "source": [
    "print('micro-f1 test', metrics.f1_score(valid_y_nmf_tfidf, clf_nmf_tfidf.predict(valid_X_nmf_tfidf), average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`nmf = NMF(200)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro-f1 test 0.5229662423907028\n"
     ]
    }
   ],
   "source": [
    "print('micro-f1 test', metrics.f1_score(valid_y_nmf_tfidf, clf_nmf_tfidf.predict(valid_X_nmf_tfidf), average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding(text, model, dim):\n",
    "    text = text.split()\n",
    "    # чтобы не доставать одно слово несколько раз\n",
    "    # сделаем счетчик, а потом векторы домножим на частоту\n",
    "    words = Counter(text)\n",
    "    total = len(text)\n",
    "    vectors = np.zeros((len(words), dim))\n",
    "    \n",
    "    for i,word in enumerate(words):\n",
    "        try:\n",
    "            v = model[word]\n",
    "            vectors[i] = v * (words[word] / total) # просто умножаем вектор на частоту\n",
    "        except (KeyError, ValueError):\n",
    "            continue\n",
    "    \n",
    "    if vectors.any():\n",
    "        vector = np.average(vectors, axis=0)\n",
    "    else:\n",
    "        vector = np.zeros((dim))\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding_tfidf(text, model, dim, documents):\n",
    "    text = text.split()\n",
    "    # чтобы не доставать одно слово несколько раз\n",
    "    # сделаем счетчик, а потом векторы домножим на частоту\n",
    "    words = Counter(text)\n",
    "    total = len(text)\n",
    "    vectors = np.zeros((len(words), dim))\n",
    "    count_docs = len(documents)\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        try:\n",
    "            v = model[word]\n",
    "            idf = math.log10(count_docs / sum([1 for i in documents if word in i]))\n",
    "            vectors[i] = v * (words[word] / total) * idf\n",
    "\n",
    "        except (KeyError, ValueError):\n",
    "            continue\n",
    "    \n",
    "    if vectors.any():\n",
    "        vector = np.average(vectors, axis=0)\n",
    "    else:\n",
    "        vector = np.zeros((dim))\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Без TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim = 50\n",
    "X_text_1_w2v = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_w2v = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_w2v[i] = get_embedding(text, w2v, dim)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_w2v[i] = get_embedding(text, w2v, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_text_w2v = np.concatenate([X_text_1_w2v, X_text_2_w2v], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X_w2v, valid_X_w2v, train_y_w2v, valid_y_w2v = train_test_split(X_text_w2v, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=100, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=500, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_w2v = RandomForestClassifier(\n",
    "    max_depth=100,\n",
    "    n_estimators=500,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "clf_w2v.fit(train_X_w2v, train_y_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro-f1 test 0.5478693967902601\n"
     ]
    }
   ],
   "source": [
    "print('micro-f1 test', metrics.f1_score(valid_y_w2v, clf_w2v.predict(valid_X_w2v), average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**С TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_1_w2v_tfidf = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_w2v_tfidf = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_w2v_tfidf[i] = get_embedding_tfidf(text, w2v, dim, data['text_1_norm'])\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_w2v_tfidf[i] = get_embedding_tfidf(text, w2v, dim, data['text_2_norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_text_w2v_tfidf = np.concatenate([X_text_1_w2v_tfidf, X_text_2_w2v_tfidf], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X_w2v_tfidf, valid_X_w2v_tfidf, train_y_w2v_tfidf, valid_y_w2v_tfidf = train_test_split(\n",
    "    X_text_w2v_tfidf, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=100, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=500, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_w2v_tfidf = RandomForestClassifier(\n",
    "    max_depth=100,\n",
    "    n_estimators=500,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "clf_w2v_tfidf.fit(train_X_w2v_tfidf, train_y_w2v_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro-f1 test 0.557830658550083\n"
     ]
    }
   ],
   "source": [
    "print('micro-f1 test', metrics.f1_score(valid_y_w2v_tfidf, clf_w2v_tfidf.predict(valid_X_w2v_tfidf), average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Без нормализации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Без TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['text_1_notnorm'] = data['text_1'].apply(tokenize)\n",
    "data['text_2_notnorm'] = data['text_2'].apply(tokenize)\n",
    "\n",
    "X_text_1_ft = np.zeros((len(data['text_1_notnorm']), dim))\n",
    "X_text_2_ft = np.zeros((len(data['text_2_notnorm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_notnorm'].values):\n",
    "    X_text_1_ft[i] = get_embedding(text, fast_text, dim)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_notnorm'].values):\n",
    "    X_text_2_ft[i] = get_embedding(text, fast_text, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_text_ft = np.concatenate([X_text_1_ft, X_text_2_ft], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X_ft, valid_X_ft, train_y_ft, valid_y_ft = train_test_split(X_text_ft, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=100, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=500, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_ft = RandomForestClassifier(\n",
    "    max_depth=100,\n",
    "    n_estimators=500,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "clf_ft.fit(train_X_ft, train_y_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`size = 50`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro-f1 test 0.5318206972883232\n"
     ]
    }
   ],
   "source": [
    "print('micro-f1 test', metrics.f1_score(valid_y_ft, clf_ft.predict(valid_X_ft), average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`size = 100`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro-f1 test 0.5279468732706143\n"
     ]
    }
   ],
   "source": [
    "print('micro-f1 test', metrics.f1_score(valid_y_ft, clf_ft.predict(valid_X_ft), average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**С TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_1_ft_tfidf = np.zeros((len(data['text_1_notnorm']), dim))\n",
    "X_text_2_ft_tfidf = np.zeros((len(data['text_2_notnorm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_notnorm'].values):\n",
    "    X_text_1_ft_tfidf[i] = get_embedding_tfidf(text, fast_text, dim, data['text_1_notnorm'])\n",
    "    \n",
    "for i, text in enumerate(data['text_2_notnorm'].values):\n",
    "    X_text_2_ft_tfidf[i] = get_embedding_tfidf(text, fast_text, dim, data['text_2_notnorm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_text_ft_tfidf = np.concatenate([X_text_1_ft_tfidf, X_text_2_ft_tfidf], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X_ft_tfidf, valid_X_ft_tfidf, train_y_ft_tfidf, valid_y_ft_tfidf = train_test_split(\n",
    "    X_text_ft_tfidf, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=100, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=500, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_ft_tfidf = RandomForestClassifier(\n",
    "    max_depth=100,\n",
    "    n_estimators=500,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "clf_ft_tfidf.fit(train_X_ft_tfidf, train_y_ft_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`size=50`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro-f1 test 0.5356945213060321\n"
     ]
    }
   ],
   "source": [
    "print('micro-f1 test', metrics.f1_score(valid_y_ft_tfidf, clf_ft_tfidf.predict(valid_X_ft_tfidf), average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`size=100`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro-f1 test 0.5323741007194245\n"
     ]
    }
   ],
   "source": [
    "print('micro-f1 test', metrics.f1_score(valid_y_ft_tfidf, clf_ft_tfidf.predict(valid_X_ft_tfidf), average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### С нормализацией"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Без TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_text_1_ft_norm = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_ft_norm = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_ft_norm[i] = get_embedding(text, fast_text, dim)\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_ft_norm[i] = get_embedding(text, fast_text, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_text_ft_norm = np.concatenate([X_text_1_ft_norm, X_text_2_ft_norm], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X_ft_norm, valid_X_ft_norm, train_y_ft_norm, valid_y_ft_norm = train_test_split(X_text_ft_norm, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=100, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=500, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_ft_norm = RandomForestClassifier(\n",
    "    max_depth=100,\n",
    "    n_estimators=500,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "clf_ft_norm.fit(train_X_ft_norm, train_y_ft_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro-f1 test 0.5428887659103486\n"
     ]
    }
   ],
   "source": [
    "print('micro-f1 test', metrics.f1_score(valid_y_ft_norm, clf_ft_norm.predict(valid_X_ft_norm), average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**С TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text_1_ft_norm_tfidf = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_ft_norm_tfidf = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_ft_norm_tfidf[i] = get_embedding_tfidf(text, fast_text, dim, data['text_1_norm'])\n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_ft_norm_tfidf[i] = get_embedding_tfidf(text, fast_text, dim, data['text_2_norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_text_ft_norm_tfidf = np.concatenate([X_text_1_ft_norm_tfidf, X_text_2_ft_norm_tfidf], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X_ft_norm_tfidf, valid_X_ft_norm_tfidf, train_y_ft_norm_tfidf, valid_y_ft_norm_tfidf = train_test_split(\n",
    "    X_text_ft_norm_tfidf, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=100, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=500, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_ft_norm_tfidf = RandomForestClassifier(\n",
    "    max_depth=100,\n",
    "    n_estimators=500,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "clf_ft_norm_tfidf.fit(train_X_ft_norm_tfidf, train_y_ft_norm_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro-f1 test 0.5268400664084117\n"
     ]
    }
   ],
   "source": [
    "print('micro-f1 test', metrics.f1_score(valid_y_ft_norm_tfidf, clf_ft_norm_tfidf.predict(valid_X_ft_norm_tfidf), \n",
    "                                        average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Все переменные с векторами:\n",
    "\n",
    "**SVD**\n",
    "\n",
    "`X_text_cv (X_text_1_cv, X_text_2_cv)\n",
    "X_text_tfidf (X_text_1_tfidf, X_text_2_tfidf)`\n",
    "\n",
    "**NMF**\n",
    "\n",
    "`X_text_nmf_cv (X_text_1_nmf_cv, X_text_2_nmf_cv)\n",
    "X_text_nmf_tfidf (X_text_1_nmf_tfidf, X_text_2_nmf_tfidf)`\n",
    "\n",
    "**Word2Vec**\n",
    "\n",
    "`X_text_w2v (X_text_1_w2v, X_text_2_w2v)\n",
    "X_text_w2v_tfidf (X_text_1_w2v_tfidf, X_text_2_w2v_tfidf)`\n",
    "\n",
    "**FastText**\n",
    "\n",
    "`X_text_ft (X_text_1_ft, X_text_2_ft)\n",
    "X_text_ft_tfidf (X_text_1_ft_tfidf, X_text_2_ft_tfidf)\n",
    "X_text_ft_norm (X_text_1_ft_norm, X_text_2_ft_norm)\n",
    "X_text_ft_norm_tfidf (X_text_1_ft_norm_tfidf, X_text_2_ft_norm_tfidf)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Косинусные близости**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(text1, text2):\n",
    "    return [float(cosine_similarity([t1], [t2])) for t1, t2 in zip(text1, text2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVD_cv = compute_cosine_similarity(X_text_1_cv, X_text_2_cv)\n",
    "SVD_tfidf = compute_cosine_similarity(X_text_1_tfidf, X_text_2_tfidf)\n",
    "NMF_cv = compute_cosine_similarity(X_text_1_nmf_cv, X_text_2_nmf_cv)\n",
    "NMF_tfidf = compute_cosine_similarity(X_text_1_nmf_tfidf, X_text_2_nmf_tfidf)\n",
    "W2V = compute_cosine_similarity(X_text_1_w2v, X_text_2_w2v)\n",
    "W2V_tfidf = compute_cosine_similarity(X_text_1_w2v_tfidf, X_text_2_w2v_tfidf)\n",
    "FT = compute_cosine_similarity(X_text_1_ft, X_text_2_ft)\n",
    "FT_tfidf = compute_cosine_similarity(X_text_1_ft_tfidf, X_text_2_ft_tfidf)\n",
    "FT_norm = compute_cosine_similarity(X_text_1_ft_norm, X_text_2_ft_norm)\n",
    "FT_norm_tfidf = compute_cosine_similarity(X_text_1_ft_norm_tfidf, X_text_2_ft_norm_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cosine_data = list(zip(SVD_cv, SVD_tfidf, NMF_cv, NMF_tfidf, W2V, W2V_tfidf, FT, FT_tfidf, FT_norm, FT_norm_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_df = pd.DataFrame(cosine_data, columns=['SVD_cv', 'SVD_tfidf', 'NMF_cv', 'NMF_tfidf', 'W2V',\n",
    "                                       'W2V_tfidf', 'FT', 'FT_tfidf', 'FT_norm', 'FT_norm_tfidf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SVD_cv</th>\n",
       "      <th>SVD_tfidf</th>\n",
       "      <th>NMF_cv</th>\n",
       "      <th>NMF_tfidf</th>\n",
       "      <th>W2V</th>\n",
       "      <th>W2V_tfidf</th>\n",
       "      <th>FT</th>\n",
       "      <th>FT_tfidf</th>\n",
       "      <th>FT_norm</th>\n",
       "      <th>FT_norm_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.483287</td>\n",
       "      <td>0.623122</td>\n",
       "      <td>0.874260</td>\n",
       "      <td>0.932015</td>\n",
       "      <td>0.936977</td>\n",
       "      <td>0.923466</td>\n",
       "      <td>0.908411</td>\n",
       "      <td>0.613016</td>\n",
       "      <td>0.842846</td>\n",
       "      <td>0.854598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.773074</td>\n",
       "      <td>0.847185</td>\n",
       "      <td>0.932658</td>\n",
       "      <td>0.954364</td>\n",
       "      <td>0.921322</td>\n",
       "      <td>0.935636</td>\n",
       "      <td>0.882652</td>\n",
       "      <td>0.662348</td>\n",
       "      <td>0.802194</td>\n",
       "      <td>0.829099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.630779</td>\n",
       "      <td>0.785893</td>\n",
       "      <td>0.962041</td>\n",
       "      <td>0.991323</td>\n",
       "      <td>0.961131</td>\n",
       "      <td>0.961387</td>\n",
       "      <td>0.951098</td>\n",
       "      <td>0.755105</td>\n",
       "      <td>0.751043</td>\n",
       "      <td>0.737266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.753070</td>\n",
       "      <td>0.688527</td>\n",
       "      <td>0.849303</td>\n",
       "      <td>0.739186</td>\n",
       "      <td>0.763902</td>\n",
       "      <td>0.727795</td>\n",
       "      <td>0.816893</td>\n",
       "      <td>0.802922</td>\n",
       "      <td>0.821385</td>\n",
       "      <td>0.827139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.992755</td>\n",
       "      <td>0.988164</td>\n",
       "      <td>0.979622</td>\n",
       "      <td>0.977436</td>\n",
       "      <td>0.914311</td>\n",
       "      <td>0.908674</td>\n",
       "      <td>0.451732</td>\n",
       "      <td>0.841454</td>\n",
       "      <td>0.611627</td>\n",
       "      <td>0.734232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     SVD_cv  SVD_tfidf    NMF_cv  NMF_tfidf       W2V  W2V_tfidf        FT  \\\n",
       "0  0.483287   0.623122  0.874260   0.932015  0.936977   0.923466  0.908411   \n",
       "1  0.773074   0.847185  0.932658   0.954364  0.921322   0.935636  0.882652   \n",
       "2  0.630779   0.785893  0.962041   0.991323  0.961131   0.961387  0.951098   \n",
       "3  0.753070   0.688527  0.849303   0.739186  0.763902   0.727795  0.816893   \n",
       "4  0.992755   0.988164  0.979622   0.977436  0.914311   0.908674  0.451732   \n",
       "\n",
       "   FT_tfidf   FT_norm  FT_norm_tfidf  \n",
       "0  0.613016  0.842846       0.854598  \n",
       "1  0.662348  0.802194       0.829099  \n",
       "2  0.755105  0.751043       0.737266  \n",
       "3  0.802922  0.821385       0.827139  \n",
       "4  0.841454  0.611627       0.734232  "
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 5420\n",
      "test 1807\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test, y_train, y_test = train_test_split(cosine_df, y, random_state=1, shuffle=True)\n",
    "print('train', df_train.shape[0])\n",
    "print('test', df_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=100, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=500, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(\n",
    "    max_depth=100,\n",
    "    n_estimators=500,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "clf.fit(df_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`svd = TruncatedSVD(200), nmf = NMF(50)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro-f1 test 0.5694521306032098\n"
     ]
    }
   ],
   "source": [
    "print('micro-f1 test', metrics.f1_score(y_test, clf.predict(df_test), average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`svd = TruncatedSVD(100), nmf = NMF(200)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro-f1 test 0.5794133923630327\n"
     ]
    }
   ],
   "source": [
    "print('micro-f1 test', metrics.f1_score(y_test, clf.predict(df_test), average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`svd = TruncatedSVD(200), nmf = NMF(200)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro-f1 test 0.5722191477587161\n"
     ]
    }
   ],
   "source": [
    "print('micro-f1 test', metrics.f1_score(y_test, clf.predict(df_test), average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`svd = TruncatedSVD(200), nmf = NMF(200), fastText(size=100)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro-f1 test 0.5744327614831212\n"
     ]
    }
   ],
   "source": [
    "print('micro-f1 test', metrics.f1_score(y_test, clf.predict(df_test), average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Итог:** лучше с **`svd = TruncatedSVD(100), nmf = NMF(200)`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изменение значения параметра `size` для `Word2Vec` и `fastText` (с 50 на 100) прироста в качестве не дало."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
